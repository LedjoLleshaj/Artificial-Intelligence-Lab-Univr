{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# Exam Planning-Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81dac69",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "Consider the following environment:\n",
    "\n",
    "<img src=\"images/env1.png\" alt=\"ex1\" style=\"zoom: 40%;\" />\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, however the agent can resist to high temperature and it can use the heat to recharge its batteries, hence it receives a positive reward for being on a lava cell. The environment is deterministic and the agent must avoid the black pits of death (cells $(0,3)$, $(1, 3)$, $(1,1)$). \n",
    "Assume that you do not have access to the motion model and to reward and that the problem is undiscounted (i.e., $\\gamma$=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy(q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection function\n",
    "    \n",
    "    Args:\n",
    "        q: q table\n",
    "        state: agent's current state\n",
    "        epsilon: epsilon parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    an = q.shape[1]\n",
    "    probs = np.full(an, epsilon / an)\n",
    "    probs[q[state].argmax()] += 1 - epsilon\n",
    "    return np.random.choice(an, p=probs)\n",
    "\n",
    "\n",
    "env = gym.make('LavaFloor-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc195b2",
   "metadata": {},
   "source": [
    "### 1.1) Given the environment reported above, find a policy with a suitable algorithm of your choice. Print the resulting policy (you can use the provided code for this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15e2e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    \"\"\"\n",
    "    Performs the Q-Learning algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon, T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "    \n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    env = environment\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        leng = 0\n",
    "        while not done:\n",
    "           #choose  action\n",
    "            action = expl_func(q, state, expl_param)\n",
    "            \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            rews[e] += reward\n",
    "            \n",
    "            max_ = q[new_state,action]\n",
    "            for a in range(environment.action_space.n):\n",
    "                if q[new_state,a] > max_:\n",
    "                    max_ = q[new_state,a]\n",
    "                \n",
    "            q[state,action] = q[state,action] + alpha * (reward + gamma *max_  - q[state,action])\n",
    "            state = new_state\n",
    "            leng+=1\n",
    "        \n",
    "        lengths[e] = leng\n",
    "            \n",
    "    policy = q.argmax(axis=1)\n",
    "    return policy, rews, lengths\n",
    "\n",
    "def sarsa(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    \"\"\"\n",
    "    Performs the SARSA algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon, T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "\n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    for i in range(episodes):\n",
    "        state = environment.reset()  # Reset the environment\n",
    "        action = expl_func(q, state, expl_param)  # Select first action\n",
    "        el = 0\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done, _ = environment.step(action)  # Execute a step\n",
    "            action_next = expl_func(q, next_state, expl_param)  # Select next action\n",
    "            q[state, action] += alpha * (reward + gamma * q[next_state, action_next] - q[state, action])  # Temporal difference\n",
    "            rews[i] += reward\n",
    "            el += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state  # Update state\n",
    "            action = action_next  # Update action\n",
    "            \n",
    "        lengths[i] = el\n",
    "        \n",
    "    policy = q.argmax(axis=1) # q.argmax(axis=1) automatically extract the policy from the q table\n",
    "    return policy, rews, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ce36292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      " [['R' 'D' 'U' 'U']\n",
      " ['D' 'L' 'U' 'L']\n",
      " ['R' 'R' 'R' 'L']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "gamma = .99\n",
    "alpha = .3\n",
    "epsilon = .1\n",
    "\n",
    "\n",
    "env = gym.make('LavaFloor-v0')\n",
    "policy, rewards, lengths = q_learning(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Policy:\\n {} \\n\".format(np.vectorize(env.actions.get)(policy.reshape(env.shape))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ae25a",
   "metadata": {},
   "source": [
    "### 1.2) Find a way to force the agent to choose the shortest path towards the goal. You can change these parameters: episodes, alpha, gamma exploration function and exploration parameter. \n",
    "\n",
    "##### Hint: you should tune a parameter to consider the immediate reward rather than the long-term one... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86059fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      " [['D' 'L' 'L' 'R']\n",
      " ['D' 'L' 'U' 'L']\n",
      " ['R' 'R' 'R' 'L']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 300 # your code here\n",
    "gamma =  .99 #your code here # lower gamma will make the robot focus on short term rews\n",
    "alpha = .3\n",
    "epsilon = .1\n",
    "\n",
    "\n",
    "# Q-Learning or SARSA epsilon greedy\n",
    "env = gym.make('LavaFloor-v0')\n",
    "p, r, l = sarsa(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Policy:\\n {} \\n\".format(np.vectorize(env.actions.get)(p.reshape(env.shape))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca6ba6",
   "metadata": {},
   "source": [
    "# 1.3) Consider and environment with states {A, B, C, D}, actions {r, l} where states {A, D} are terminal. Consider the following sequence of learning episodes:\n",
    "* E1: (B, r, C, −1),(C, r, C, −1),(C, r, D, +1)\n",
    "* E2: (B, r, C, −1),(C, r, D, +1)\n",
    "* E3: (B, l, A, +5)\n",
    "* E4: (B, l, B, −1),(B, l, B, −1),(B, l, A, +5)\n",
    "### Compute v(s) for all non-terminal states by using a sample-based evaluation approach (i.e., computing the values with the function reported below). Assume $\\alpha$ = .5 and $\\gamma$ = 1.\n",
    "### $V(s) = (1- \\alpha) \\cdot V(s) + \\alpha \\cdot (r + \\gamma \\cdot V'(s'))$ \n",
    "### where $s$ is the state under consideration (first element of each tuple), $s'$  (third element of each tuple ) the state reached by starting from $s$ and performing the action $a$ (second element of each tuple). And finally, $r$ is the reward (last element of each tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = {1: [('B', 'r', 'C', -1), ('C', 'r', 'C', -1),('C', 'r', 'D', 1)], \n",
    "            2: [('B', 'r', 'C', -1), ('C', 'r', 'D', 1)], \n",
    "            3: [('B', 'l', 'A', 5)], \n",
    "            4: [('B', 'l', 'B', -1),('B', 'l', 'B', -1),('B', 'l', 'A', 5)]}\n",
    "\n",
    "v = {'A': 5, 'B': 0, 'C': 0, 'D': 1}\n",
    "alpha = 0.5\n",
    "gamma = 1\n",
    "\n",
    "\n",
    "for episode, values in episodes.items():\n",
    "    for tuple in values:\n",
    "        # your code here\n",
    "        ...\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc8f7c",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5f593",
   "metadata": {},
   "source": [
    "Consider the figure below where **S=$(1,2)$** and **G=$(3,1)$** are the starting and goal positions respectively. Consider the problem of finding a minimum cost path from S to G assuming the agent can move in the four directions (if there is no\n",
    "obstacle) and that each movement has a unitary cost. The environment is deterministic. Answer the following questions:\n",
    "\n",
    "<img src=\"images/ex2.png\" alt=\"ex2\" style=\"zoom: 40%;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ba51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062b99",
   "metadata": {},
   "source": [
    "### 2.1) Verify by using the code developed in the lab that the Manhattan distance (l1_norm) is a  *consistent* heuristic in this environment. In particular, you should implement a function that checks whether for every couple of state $(s,s')$ (where $s'$ is a successor state of $s$), the consistency condition is verified. The function should return true if the heuristic is consistent and false otherwise. Recall that every action has a cost of 1...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0352c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97399378",
   "metadata": {},
   "source": [
    "### 2.2) Consider the A* algorithm and assume want to achieve optimality. Based on the *consistent* heuristics of Section 2.1, state whether it is best to use a graph search or tree search strategy. Motivate your answer and show the results of A* execution and the differences between the two versions in terms of the computed solution, number of nodes generated and maximum number of nodes in memory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217679b5",
   "metadata": {},
   "source": [
    "Since we have a consistent heuristic we have to choose a graph search version to achieve optimality. We know only if tree search + admissible heuristic or graph search + consistent heuristic => optimality of A*. In all other situations, we cannot guarantee optimality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ec4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_with_higher_cost(queue, node):\n",
    "    if node.state in queue:\n",
    "        if queue[node.state].value > node.value: \n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425883ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "solution_ts, time_ts, memory_ts = astar_tree_search(env)\n",
    "solution_gs, time_gs, memory_gs = astar_graph_search(env)\n",
    "\n",
    "print(\"Solution (tree-search): {}\".format(solution_2_string(solution_ts, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_ts))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_ts))\n",
    "print('='*65)\n",
    "print(\"\\nSolution (graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177b11c",
   "metadata": {},
   "source": [
    "### 2.3) Let us consider BFS, show the path of the optimal solution (avoiding the repetition of states) to achieve the goal. In this scenario can we guarantee that the returned solution is the least cost one? If we used Iterative Deepening Search(IDS) with the same methodology used for BFS can we guarantee a least cost solution? Justify your answer and show the results of the different strategies by using the code developed during the lab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your BFS code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c11d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "solution_gs, time_gs, memory_gs = BFS_GraphSearch(env)\n",
    "print(\"Solution (BFS graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bfe7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example IDS+GRAPH SEARCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41388738",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "solution_gs, time_gs, memory_gs, iterations_gs = IDS(env, Recursive_DLS_GraphSearch)\n",
    "print(\"Solution (IDS graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f593b",
   "metadata": {},
   "source": [
    "### Discuss the results achieved with respect to the 2.3 question:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0e580",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9bc2c64",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20f428",
   "metadata": {},
   "source": [
    "Consider the environment displayed in Figure below, where states $(0, 3)$ and $(1, 3)$ are terminal states with reward $+1$ and $−1$ respectively. The agent can move in the four directions. The transition model states that for every state and action the agent has $0.8$ chances of moving in the chosen direction and $0.1$ chances to move in the othogonal directions.The reward model states that for every state, action and successor state the agent pays $−0.04$. Assume that the dicount factor is set to $0.9$. Answer the following questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b78047",
   "metadata": {},
   "source": [
    "<img src=\"images/ex3.1.png\" alt=\"ex3\" style=\"zoom: 40%;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('VeryBadLavaFloor-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac451e5",
   "metadata": {},
   "source": [
    "### 3.1) Use one of the methods developed in the lab to compute the value function for the left diagram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da933217",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('VeryBadLavaFloor-v0')\n",
    "policy, values = ... # your code here\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8216357",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66795ad",
   "metadata": {},
   "source": [
    "<img src=\"images/ex3.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180254a",
   "metadata": {},
   "source": [
    "### 3.2) Consider the right diagram and focus on states $(2, 1)$. State whether the action reported in the right diagram (the blue one) represents the optimal action for that state. Motivate your answer with the code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebad340",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "value_function = [0.50939438, 0.64958568, 0.79536209, 1, 0.39844322, 0, 0.48644002, -1, 0.29628832,  0.253867, 0.34475423, 0.12987275]\n",
    "id_start_state = 9\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "values_ex = [0, 0, 0, 0]\n",
    "'''\n",
    "YOUR CODE HERE\n",
    "'''\n",
    "    \n",
    "print(np.asarray(values_ex))\n",
    "print(f'The correct action to perform should be: {actions[np.argmax(values_ex)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fa5fc",
   "metadata": {},
   "source": [
    "### 3.3) Compute the probability of ending in state (1, 3) if we execute the sequence of actons < Up, Up > from state (2, 2). Motivate your answer reporting the code and the solution printed. The following image shows a diagram to guide the solution process as a hint for you:\n",
    "\n",
    "\n",
    "<img src=\"images/example.png\" alt=\"ex3\" style=\"zoom: 30%;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_start_state = 10\n",
    "state = id_start_state\n",
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "prob = 0\n",
    "action = 2\n",
    "probs_fin = 0\n",
    "\n",
    "for next_state in range(env.observation_space.n):\n",
    "    \n",
    "    if env.T[state, action, next_state] == 0:\n",
    "        continue\n",
    "\n",
    "\n",
    "    # your code here\n",
    "    ...\n",
    "\n",
    "    for second_next_state in range(env.observation_space.n):\n",
    "        \n",
    "        if env.T[next_state, action, second_next_state] == 0:\n",
    "            continue\n",
    "\n",
    "        if env.grid[second_next_state] == \"P\": #check that the state is (1,3) i.e., the black pit\n",
    "            # your code here\n",
    "            ...\n",
    "\n",
    "            print(f'{env.state_to_pos(state)} --> {env.state_to_pos(next_state)} --> {env.state_to_pos(second_next_state)}')\n",
    "            print(f'probs: {env.T[state, action, next_state]} --> {env.T[next_state, action, second_next_state]}')\n",
    "\n",
    "            # your code here\n",
    "            ...\n",
    "\n",
    "            print('================')\n",
    "            break\n",
    "    \n",
    "print()\n",
    "print('Probability: ', probs_fin)  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c0594",
   "metadata": {},
   "source": [
    "### 3.4) Consider the following environment where states $(0, 3)$ and $(1, 3)$ are terminal states with reward $+1$ and $−1$ respectively. Assume the transition model is the same one defined above and that the discounted factor is $0.9$ as above. However, now the agent pays $−0.4$ instead of $−0.04$ for every action, state and successor state.  Compute the new value function and the optimal policy for this new environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89237f75",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/env_3.4.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ca1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('NiceLavaFloor-v0')\n",
    "policy, values = ... # yout code here\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4b4e0",
   "metadata": {},
   "source": [
    "### 3.5) Modify the value iteration parameters so that the policy allows the agent to reach the goal only starting from states $(0,1)$, $(0,2)$ and $(1,2)$, as reported in the image below . Motivate your answer.\n",
    "\n",
    "\n",
    "#### hint: given the negative reward the agent should care about the immediate reward and not the long-term reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd75f5",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/result_ex3.5.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('NiceLavaFloor-v0')\n",
    "policy, values =  ... #your code here\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(policy_render)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "703075dc036e8ebc3a027aec30cd295176a007527fa40434b7705e84e779ac0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
